{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping of RI UFRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T21:58:56.280928Z",
     "start_time": "2023-11-12T21:58:56.258575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing the required libraries.\n",
    "import scrapy, csv, re, pandas as pd, numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Spider class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.IGNORECASE).strip()\n",
    "    text = text.replace(\"- \", \"-\").replace(\"\\ufeff\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the data repository.\n",
    "data = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the URL.\n",
    "url_base = \"https://repositorio.ufrn.br/handle/123456789/11949\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T21:58:56.287892Z",
     "start_time": "2023-11-12T21:58:56.286402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of Spider class.\n",
    "class Spider_RI_UFRN(scrapy.Spider):\n",
    "    name = \"scraper_ri_ufrn\"\n",
    "\n",
    "    # Start point to run the spider.\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_base, callback=self.parse_categories)\n",
    "\n",
    "    def parse_categories(self, response):\n",
    "        # Getting the relative URLs of the categories (PhD and MSc' Thesis).\n",
    "        css = \"#content > div:nth-child(3) > div > div.col-md-9 > div.container.row > div > div.list-group\"\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        soup = soup.select(css).pop()\n",
    "        urls = [(item.string.strip(), item[\"href\"]) for item in soup.select(\"a\")]\n",
    "        for url in urls:\n",
    "            yield response.follow(url=url[1], callback=self.parse_thesis_links,\n",
    "                                  meta={\"category\": url[0]})\n",
    "\n",
    "    def parse_thesis_links(self, response):\n",
    "        category = response.meta[\"category\"] if \"category\" in response.meta else None\n",
    "        links = response.meta[\"links\"] if \"links\" in response.meta else dict()\n",
    "\n",
    "        # Getting the relative URLs of the documents.\n",
    "        if category not in links:\n",
    "            links[category] = list()\n",
    "        css = \"#content > div:nth-child(3) > div > div.col-md-9 > table\"\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        urls = soup.select(css).pop()\n",
    "        urls = [item[\"href\"] for item in urls.find_all(\"a\") if item.has_attr(\"href\")]\n",
    "        if len(urls) > 0:\n",
    "            links[category].extend(urls)\n",
    "\n",
    "        # Navigating among the next pages.\n",
    "        css = \"#content > div:nth-child(3) > div > div.col-md-9 > div:nth-child(7)\"\n",
    "        soup = soup.select(css).pop()\n",
    "        url = soup.find_all(\"a\", string=re.compile(r\"(next|prÃ³ximo)\", flags=re.IGNORECASE))\n",
    "        if len(url) > 0:\n",
    "            url = url[0][\"href\"]\n",
    "            yield response.follow(url=url, callback=self.parse_thesis_links,\n",
    "                                  meta={\"category\": category, \"links\": links})\n",
    "        else:\n",
    "            for url in links[category]:\n",
    "                yield response.follow(url=url, callback=self.parse_data,\n",
    "                                      meta={\"category\": category})\n",
    "\n",
    "    def parse_data(self, response):\n",
    "        category = response.meta[\"category\"] if \"category\" in response.meta else None\n",
    "\n",
    "        # Getting the HTML of page.\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        css = \"table.table.itemDisplayTable\"\n",
    "        html = soup.select(css).pop()\n",
    "\n",
    "        # Extracting the data.\n",
    "        record = {\"category\": category}\n",
    "        try:\n",
    "            for tag in html.select(\"table > tr\")[:-1]:\n",
    "                label = tag.select_one(\"tr > td.metadataFieldLabel\")\n",
    "                label = label.text.strip().lower().replace(\":\", \"\")\n",
    "                record[label] = tag.select_one(\"tr > td.metadataFieldValue\")\n",
    "                record[label] = record[label].text.strip().replace(\"Resumo\", \"\") \\\n",
    "                    if record[label] is not None else None\n",
    "            css = \"div.panel.panel-info > table > tr:nth-child(2) > td:first-child > a\"\n",
    "            record[\"document_url\"] = soup.select_one(css)[\"href\"]\n",
    "            record[\"document_url\"] = urljoin(url_base, record[\"document_url\"])\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR-DEBUG: error in extract the feature {label} in {response.url}\")\n",
    "        data.append(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executing the Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T21:59:44.880213Z",
     "start_time": "2023-11-12T21:58:56.294390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Execution Process to run the spider.\n",
    "process = CrawlerProcess()\n",
    "process.crawl(Spider_RI_UFRN)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data.\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataframe.\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the five first records.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the None values.\n",
    "df.replace({np.nan: None}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns.\n",
    "df.rename(columns={\"keywords\": \"auth_keywords\", \"issue date\": \"defense_date\",\n",
    "                   \"portuguese abstract\": \"pt_abstract\", \"abstract\": \"en_abstract\",\n",
    "                   \"other titles\": \"col1\", \"embargoed until\": \"col2\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unecessary columns.\n",
    "df.drop(columns=[\"col1\", \"col2\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the column \"category\".\n",
    "df.category = df.category.apply(lambda x: \"PhD\" if \"Doutorado\" in x else \\\n",
    "    \"MSc\" if \"Mestrado\" in x else \"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the columns \"authors\" and \"advisor\".\n",
    "df.loc[df.authors.notnull(), \"authors\"] = df.loc[df.authors.notnull(), \"authors\"].apply(\n",
    "    lambda x: f'{x.split(\",\")[1].strip()} {x.split(\",\")[0].strip()}')\n",
    "df.loc[df.advisor.notnull(), \"advisor\"] = df.loc[df.advisor.notnull(), \"advisor\"].apply(\n",
    "    lambda x: f'{x.split(\",\")[1].strip()} {x.split(\",\")[0].strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the column \"auth_keywords\".\n",
    "df.loc[df.auth_keywords.notnull(), \"auth_keywords\"] = df.loc[\n",
    "    df.auth_keywords.notnull(), \"auth_keywords\"].apply(lambda x: tuple(\n",
    "        [clean_text(k).strip() for k in x.split(\";\")\n",
    "            if len(clean_text(k).strip())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the columns \"title\", \"citation\", \"pt_abstract\" and \"en_abstract\".\n",
    "df.loc[df.title.notnull(), \"title\"] = df.loc[\n",
    "    df.title.notnull(), \"title\"].apply(clean_text)\n",
    "df.loc[df.citation.notnull(), \"citation\"] = df.loc[\n",
    "    df.citation.notnull(), \"citation\"].apply(clean_text)\n",
    "df.loc[df.pt_abstract.notnull(), \"pt_abstract\"] = df.loc[\n",
    "    df.pt_abstract.notnull(), \"pt_abstract\"].apply(clean_text)\n",
    "df.loc[df.en_abstract.notnull(), \"en_abstract\"] = df.loc[\n",
    "    df.en_abstract.notnull(), \"en_abstract\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the result.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T21:59:44.880941Z",
     "start_time": "2023-11-12T21:59:44.854963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the data into a CSV file.\n",
    "df.to_csv(\"ppgeec_phd_msc_thesis.csv\", header=0, index=False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
